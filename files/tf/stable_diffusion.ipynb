{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion (Keras)\n",
    "\n",
    "[![Open in Colab](https://lab.aef.me/files/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/tf/stable_diffusion.ipynb)\n",
    "[![Open in Kaggle](https://lab.aef.me/files/assets/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/adamelliotfields/lab/blob/main/files/tf/stable_diffusion.ipynb)\n",
    "[![Render nbviewer](https://lab.aef.me/files/assets/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/tf/stable_diffusion.ipynb)\n",
    "\n",
    "> By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M [license](https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE).\n",
    "\n",
    "Complete Stable Diffusion 1.4 implementation based on the official KerasCV [model](https://github.com/keras-team/keras-cv/tree/master/keras_cv/src/models/stable_diffusion), which itself is based on [`stable-diffusion-tensorflow`](https://github.com/divamgupta/stable-diffusion-tensorflow). Also includes Keras implementations of [CLIP](https://github.com/openai/CLIP) and the [DDPM](https://arxiv.org/abs/2006.11239) scheduler from [Diffusers](https://github.com/huggingface/diffusers/blob/v0.3.0/src/diffusers/schedulers/scheduling_ddpm.py).\n",
    "\n",
    "Uses the Keras 3 [Ops](https://keras.io/api/ops/) API which works across backends (TensorFlow, JAX, PyTorch). Unlike other KerasCV pretrained models, the weights aren't hosted on Kaggle; they're a port of the [original](https://huggingface.co/CompVis/stable-diffusion-v1-4) PyTorch weights on [Hugging Face](https://huggingface.co/fchollet/stable-diffusion).\n",
    "\n",
    "The code is organized so each component has its own cell with layers first then models and finally the Stable Diffusion pipeline itself.\n",
    "\n",
    "**Changelog**\n",
    "\n",
    "* v1 only\n",
    "* default JIT-compilation to `\"auto\"`\n",
    "* cache weights in Google Drive when in Colab\n",
    "* use DDPM params from [tinygrad](https://github.com/tinygrad/tinygrad/blob/master/examples/stable_diffusion.py) implementation\n",
    "* move `batch_size` to pipeline constructor\n",
    "* remove lazy initialization in pipeline constructor\n",
    "* remove `download_weights` keyword argument from models\n",
    "* rename `unconditional_guidance_scale` to `guidance_scale`\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* [ ] Annotations and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from importlib.util import find_spec\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "if find_spec(\"google.colab\"):\n",
    "    subprocess.run([\"pip\", \"install\", \"-qU\", \"keras\"])\n",
    "    # cache ~4GB in Google Drive:\n",
    "    # * bpe_simple_vocab_16e6.txt.gz (1.3M)\n",
    "    # * kcv_decoder.h5 (189M)\n",
    "    # * kcv_diffusion_model.h5 (3.3G)\n",
    "    # * kcv_encoder.h5 (470M)\n",
    "    # * vae_encoder.h5 (131M)\n",
    "    CACHE_DIR = \"/content/drive/MyDrive/keras\"\n",
    "else:\n",
    "    CACHE_DIR = os.environ.get(\"KERAS_HOME\", os.path.expanduser(\"~/.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import html\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "from functools import lru_cache\n",
    "from keras import (\n",
    "    Model,\n",
    "    Sequential,\n",
    "    activations,\n",
    "    config,\n",
    "    layers,\n",
    "    ops,\n",
    "    random,\n",
    "    utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Config\n",
    "EPSILON = 1e-5\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "GLOBAL_DTYPE = \"mixed_float16\"  # mixed_float16 is 2-3x faster than float32\n",
    "CACHE_SUBDIR = \"models/stable_diffusion_v1\"\n",
    "MAX_PROMPT_LENGTH = 77  # CLIP's limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions\n",
    "def td_dot(a, b):\n",
    "    aa = ops.reshape(a, (-1, a.shape[2], a.shape[3]))\n",
    "    bb = ops.reshape(b, (-1, b.shape[2], b.shape[3]))\n",
    "    cc = layers.Dot(axes=(2, 1))([aa, bb])\n",
    "    return ops.reshape(cc, (-1, a.shape[1], cc.shape[1], cc.shape[2]))\n",
    "\n",
    "\n",
    "def quick_gelu(x):\n",
    "    return x * ops.sigmoid(x * 1.702)\n",
    "\n",
    "\n",
    "def display_images(images, horizontal=False):\n",
    "    if horizontal:\n",
    "        total_width = sum(img.shape[1] for img in images)\n",
    "        max_height = max(img.shape[0] for img in images)\n",
    "        combined_image = PILImage.new(\"RGB\", (total_width, max_height))\n",
    "        offset = 0\n",
    "        for img in images:\n",
    "            img = PILImage.fromarray(img)\n",
    "            combined_image.paste(img, (offset, 0))\n",
    "            offset += img.width\n",
    "        display(combined_image)\n",
    "    else:\n",
    "        for image in images:\n",
    "            display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CLIPTokenizer\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"Return a list of utf-8 bytes and a corresponding list of unicode strings.\"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
    "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
    "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class CLIPTokenizer:\n",
    "    def __init__(self):\n",
    "        bpe_path = utils.get_file(\n",
    "            cache_dir=CACHE_DIR,\n",
    "            cache_subdir=CACHE_SUBDIR,\n",
    "            # file_hash=\"924691ac288e54409236115652ad4aa250f48203de50a9e4722a6ecd48d6804a\",\n",
    "            origin=\"https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz\",\n",
    "        )\n",
    "\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n",
    "        merges = merges[1 : 49152 - 256 - 2 + 1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v + \"</w>\" for v in vocab]\n",
    "\n",
    "        for merge in merges:\n",
    "            vocab.append(\"\".join(merge))\n",
    "\n",
    "        vocab.extend([\"<|startoftext|>\", \"<|endoftext|>\"])\n",
    "        self.vocab = vocab\n",
    "        self.encoder = self._create_encoder(self.vocab)\n",
    "        self.decoder = self._create_decoder(self.encoder)\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
    "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
    "        }\n",
    "        self.cache = {\n",
    "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
    "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
    "        }\n",
    "        self.pat = self._create_pat()\n",
    "\n",
    "    def _create_encoder(self, vocab):\n",
    "        return dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    def _create_decoder(self, encoder):\n",
    "        return {v: k for k, v in encoder.items()}\n",
    "\n",
    "    def _create_pat(self):\n",
    "        return re.compile(\n",
    "            \"|\".join([re.escape(key) for key in self.special_tokens.keys()])\n",
    "            + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def end_of_text(self):\n",
    "        return self.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "    @property\n",
    "    def start_of_text(self):\n",
    "        return self.encoder[\"<|startoftext|>\"]\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = [tokens]\n",
    "\n",
    "        tokens_added = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                continue\n",
    "            tokens_added += 1\n",
    "            self.vocab.append(token)\n",
    "            self.special_tokens[token] = token\n",
    "            self.cache[token] = token\n",
    "\n",
    "        self.encoder = self._create_encoder(self.vocab)\n",
    "        self.decoder = self._create_decoder(self.encoder)\n",
    "        self.pat = self._create_pat()\n",
    "        return tokens_added\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
    "        pairs = get_pairs(word)\n",
    "        if not pairs:\n",
    "            return token + \"</w>\"\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except ValueError:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return [self.start_of_text] + bpe_tokens + [self.end_of_text]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text])\n",
    "        text = text.decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DDPMScheduler\n",
    "class DDPMScheduler:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_timesteps: int, number of diffusion steps used to train the model. Defaults to 1000.\n",
    "        beta_start: float, the starting `beta` value of inference. Defaults to 0.0001.\n",
    "        beta_end: float, the final `beta` value. Defaults to 0.02.\n",
    "        beta_schedule: \"linear\" or \"scaled_linear\", a mapping from a beta range to a sequence of betas for stepping the model. Defaults to \"linear\".\n",
    "        variance_type: \"fixed_small\", \"fixed_small_log\", \"fixed_large\", \"fixed_large_log\", \"learned\" or \"learned_range\", options to clip the variance used when adding noise to the de-noised sample. Defaults to \"fixed_small\".\n",
    "        clip_sample: bool, option to clip predicted sample between -1 and 1 for numerical stability. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        beta_schedule=\"linear\",\n",
    "        variance_type=\"fixed_small\",\n",
    "        clip_sample=True,\n",
    "    ):\n",
    "        self.train_timesteps = train_timesteps\n",
    "\n",
    "        if beta_schedule == \"linear\":\n",
    "            self.betas = ops.linspace(beta_start, beta_end, train_timesteps)\n",
    "        elif beta_schedule == \"scaled_linear\":\n",
    "            # this schedule is very specific to the latent diffusion model\n",
    "            self.betas = ops.linspace(beta_start**0.5, beta_end**0.5, train_timesteps) ** 2\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid beta schedule: {beta_schedule}.\")\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = ops.cumprod(self.alphas)\n",
    "        self.variance_type = variance_type\n",
    "        self.clip_sample = clip_sample\n",
    "        self.seed_generator = random.SeedGenerator(seed=42)\n",
    "\n",
    "    def _get_variance(self, timestep, predicted_variance=None):\n",
    "        alpha_prod = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_prev = self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0\n",
    "        variance = (1 - alpha_prod_prev) / (1 - alpha_prod) * self.betas[timestep]\n",
    "\n",
    "        if self.variance_type == \"fixed_small\":\n",
    "            variance = ops.clip(variance, x_min=1e-20, x_max=1)\n",
    "        elif self.variance_type == \"fixed_small_log\":\n",
    "            variance = ops.log(ops.clip(variance, x_min=1e-20, x_max=1))\n",
    "        elif self.variance_type == \"fixed_large\":\n",
    "            variance = self.betas[timestep]\n",
    "        elif self.variance_type == \"fixed_large_log\":\n",
    "            variance = ops.log(self.betas[timestep])\n",
    "        elif self.variance_type == \"learned\":\n",
    "            return predicted_variance\n",
    "        elif self.variance_type == \"learned_range\":\n",
    "            min_log = variance\n",
    "            max_log = self.betas[timestep]\n",
    "            frac = (predicted_variance + 1) / 2\n",
    "            variance = frac * max_log + (1 - frac) * min_log\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid variance type: {self.variance_type}\")\n",
    "        return variance\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output,\n",
    "        timestep,\n",
    "        sample,\n",
    "        predict_epsilon=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion process from the learned model outputs (usually the predicted noise).\n",
    "        Args:\n",
    "            model_output: a Tensor containing direct output from learned diffusion model.\n",
    "            timestep: current discrete timestep in the diffusion chain.\n",
    "            sample: a Tensor containing the current instance of sample being created by diffusion process.\n",
    "            predict_epsilon: bool, whether the model is predicting noise (epsilon) or samples. Defaults to True.\n",
    "        Returns:\n",
    "            The predicted sample at the previous timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\n",
    "            \"learned\",\n",
    "            \"learned_range\",\n",
    "        ]:\n",
    "            model_output, predicted_variance = ops.split(model_output, sample.shape[1], axis=1)\n",
    "        else:\n",
    "            predicted_variance = None\n",
    "\n",
    "        # 1. compute alphas, betas\n",
    "        alpha_prod = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_prev = self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0\n",
    "        beta_prod = 1 - alpha_prod\n",
    "        beta_prod_prev = 1 - alpha_prod_prev\n",
    "\n",
    "        # 2. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        if predict_epsilon:\n",
    "            sqrt_alpha = alpha_prod**0.5\n",
    "            sqrt_beta = beta_prod**0.5\n",
    "            pred_original_sample = (sample - sqrt_beta * model_output) / sqrt_alpha\n",
    "        else:\n",
    "            pred_original_sample = model_output\n",
    "\n",
    "        # 3. Clip \"predicted x_0\"\n",
    "        if self.clip_sample:\n",
    "            pred_original_sample = ops.clip_by_value(pred_original_sample, -1, 1)\n",
    "\n",
    "        # 4. Compute coefficients for pred_original_sample x_0 and current\n",
    "        # sample x_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        sqrt_alpha_prev = alpha_prod_prev**0.5\n",
    "        sqrt_alphas_timestep = self.alphas[timestep] ** 0.5\n",
    "        pred_original_sample_coeff = (sqrt_alpha_prev * self.betas[timestep]) / beta_prod\n",
    "        current_sample_coeff = sqrt_alphas_timestep * beta_prod_prev / beta_prod\n",
    "\n",
    "        # 5. Compute predicted previous sample µ_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_prev_sample = (\n",
    "            pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n",
    "        )\n",
    "\n",
    "        # 6. Add noise\n",
    "        variance = 0\n",
    "        if timestep > 0:\n",
    "            noise = random.normal(model_output.shape, seed=self.seed_generator)\n",
    "            variance = self._get_variance(timestep, predicted_variance=predicted_variance)\n",
    "            variance = (variance**0.5) * noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "        return pred_prev_sample\n",
    "\n",
    "    def add_noise(\n",
    "        self,\n",
    "        original_samples,\n",
    "        noise,\n",
    "        timesteps,\n",
    "    ):\n",
    "        sqrt_alpha_prod = ops.take(self.alphas_cumprod, timesteps) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = (1 - ops.take(self.alphas_cumprod, timesteps)) ** 0.5\n",
    "\n",
    "        for _ in range(3):\n",
    "            sqrt_alpha_prod = ops.expand_dims(sqrt_alpha_prod, axis=-1)\n",
    "            sqrt_one_minus_alpha_prod = ops.expand_dims(sqrt_one_minus_alpha_prod, axis=-1)\n",
    "\n",
    "        sqrt_alpha_prod = ops.cast(sqrt_alpha_prod, dtype=original_samples.dtype)\n",
    "        sqrt_one_minus_alpha_prod = ops.cast(sqrt_one_minus_alpha_prod, dtype=noise.dtype)\n",
    "        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        return noisy_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PaddedConv2D\n",
    "class PaddedConv2D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, padding=0, strides=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding2d = layers.ZeroPadding2D(padding)\n",
    "        self.conv2d = layers.Conv2D(filters, kernel_size, strides=strides, padding=\"valid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.padding2d(inputs)\n",
    "        return self.conv2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ResnetBlock\n",
    "class ResnetBlock(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.norm1 = layers.GroupNormalization(epsilon=EPSILON)\n",
    "        self.conv1 = PaddedConv2D(output_dim, 3, padding=1)\n",
    "        self.norm2 = layers.GroupNormalization(epsilon=EPSILON)\n",
    "        self.conv2 = PaddedConv2D(output_dim, 3, padding=1)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[-1] != self.output_dim:\n",
    "            self.residual_proj = PaddedConv2D(self.output_dim, 1)\n",
    "        else:\n",
    "            self.residual_proj = lambda x: x\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(activations.swish(self.norm1(inputs)))\n",
    "        x = self.conv2(activations.swish(self.norm2(x)))\n",
    "        return x + self.residual_proj(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title AttentionBlock\n",
    "class AttentionBlock(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.norm = layers.GroupNormalization(epsilon=EPSILON)\n",
    "        self.query = PaddedConv2D(output_dim, 1)\n",
    "        self.key = PaddedConv2D(output_dim, 1)\n",
    "        self.value = PaddedConv2D(output_dim, 1)\n",
    "        self.output_proj = PaddedConv2D(output_dim, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm(inputs)\n",
    "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
    "\n",
    "        # attention\n",
    "        shape = ops.shape(q)\n",
    "        h, w, c = shape[1], shape[2], shape[3]\n",
    "        q = ops.reshape(q, (-1, h * w, c))  # b, hw, c\n",
    "        k = ops.transpose(k, (0, 3, 1, 2))\n",
    "        k = ops.reshape(k, (-1, c, h * w))  # b, c, hw\n",
    "        y = q @ k\n",
    "        y = y * 1 / ops.sqrt(ops.cast(c, self.compute_dtype))\n",
    "        y = activations.softmax(y)\n",
    "\n",
    "        # values\n",
    "        v = ops.transpose(v, (0, 3, 1, 2))\n",
    "        v = ops.reshape(v, (-1, c, h * w))\n",
    "        y = ops.transpose(y, (0, 2, 1))\n",
    "        x = v @ y\n",
    "        x = ops.transpose(x, (0, 2, 1))\n",
    "        x = ops.reshape(x, (-1, h, w, c))\n",
    "        return self.output_proj(x) + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CrossAttention\n",
    "class CrossAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, head_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = num_heads * head_size\n",
    "        self.query = layers.Dense(self.channels, use_bias=False)\n",
    "        self.key = layers.Dense(self.channels, use_bias=False)\n",
    "        self.value = layers.Dense(self.channels, use_bias=False)\n",
    "        self.scale = head_size**-0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.output_proj = layers.Dense(self.channels)\n",
    "\n",
    "    def call(self, inputs, context=None):\n",
    "        if context is None:\n",
    "            context = inputs\n",
    "\n",
    "        q, k, v = self.query(inputs), self.key(context), self.value(context)\n",
    "\n",
    "        q = ops.reshape(q, (-1, inputs.shape[1], self.num_heads, self.head_size))\n",
    "        k = ops.reshape(k, (-1, context.shape[1], self.num_heads, self.head_size))\n",
    "        v = ops.reshape(v, (-1, context.shape[1], self.num_heads, self.head_size))\n",
    "\n",
    "        q = ops.transpose(q, (0, 2, 1, 3))  # (batch_size, num_heads, time, head_size)\n",
    "        k = ops.transpose(k, (0, 2, 3, 1))  # (batch_size, num_heads, head_size, time)\n",
    "        v = ops.transpose(v, (0, 2, 1, 3))  # (batch_size, num_heads, time, head_size)\n",
    "\n",
    "        score = td_dot(q, k) * self.scale\n",
    "        weights = activations.softmax(score)  # (batch_size, num_heads, time, time)\n",
    "        attn = td_dot(weights, v)\n",
    "        attn = ops.transpose(attn, (0, 2, 1, 3))  # (batch_size, time, num_heads, head_size)\n",
    "        output = ops.reshape(attn, (-1, inputs.shape[1], self.channels))\n",
    "        return self.output_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GEGLU\n",
    "class GEGLU(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.dense = layers.Dense(output_dim * 2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x, gate = x[..., : self.output_dim], x[..., self.output_dim :]\n",
    "        tanh_res = activations.tanh(gate * 0.7978845608 * (1 + 0.044715 * (gate**2)))\n",
    "        return x * 0.5 * gate * (1 + tanh_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title BasicTransformerBlock\n",
    "class BasicTransformerBlock(layers.Layer):\n",
    "    def __init__(self, dim, num_heads, head_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=EPSILON)\n",
    "        self.attn1 = CrossAttention(num_heads, head_size)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=EPSILON)\n",
    "        self.attn2 = CrossAttention(num_heads, head_size)\n",
    "        self.norm3 = layers.LayerNormalization(epsilon=EPSILON)\n",
    "        self.geglu = GEGLU(dim * 4)\n",
    "        self.dense = layers.Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_inputs, context = inputs\n",
    "        x = self.attn1(self.norm1(x_inputs), context=None) + x_inputs\n",
    "        x = self.attn2(self.norm2(x), context=context) + x\n",
    "        return self.dense(self.geglu(self.norm3(x))) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SpatialTransformer\n",
    "class SpatialTransformer(layers.Layer):\n",
    "    def __init__(self, num_heads, head_size, fully_connected=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm = layers.GroupNormalization(epsilon=EPSILON)\n",
    "        channels = num_heads * head_size\n",
    "\n",
    "        if fully_connected:\n",
    "            self.proj1 = layers.Dense(channels)\n",
    "        else:\n",
    "            self.proj1 = PaddedConv2D(channels, 1)\n",
    "\n",
    "        self.transformer_block = BasicTransformerBlock(channels, num_heads, head_size)\n",
    "\n",
    "        if fully_connected:\n",
    "            self.proj2 = layers.Dense(channels)\n",
    "        else:\n",
    "            self.proj2 = PaddedConv2D(channels, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_inputs, context = inputs\n",
    "        _, h, w, c = x_inputs.shape\n",
    "        x = self.norm(x_inputs)\n",
    "        x = self.proj1(x)\n",
    "        x = ops.reshape(x, (-1, h * w, c))\n",
    "        x = self.transformer_block([x, context])\n",
    "        x = ops.reshape(x, (-1, h, w, c))\n",
    "        return self.proj2(x) + x_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ResBlock\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.entry_flow = [\n",
    "            layers.GroupNormalization(epsilon=EPSILON),\n",
    "            layers.Activation(\"swish\"),\n",
    "            PaddedConv2D(output_dim, 3, padding=1),\n",
    "        ]\n",
    "        self.embedding_flow = [\n",
    "            layers.Activation(\"swish\"),\n",
    "            layers.Dense(output_dim),\n",
    "        ]\n",
    "        self.exit_flow = [\n",
    "            layers.GroupNormalization(epsilon=EPSILON),\n",
    "            layers.Activation(\"swish\"),\n",
    "            PaddedConv2D(output_dim, 3, padding=1),\n",
    "        ]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[0][-1] != self.output_dim:\n",
    "            self.residual_proj = PaddedConv2D(self.output_dim, 1)\n",
    "        else:\n",
    "            self.residual_proj = lambda x: x\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_inputs, embeddings = inputs\n",
    "        x = x_inputs\n",
    "        for layer in self.entry_flow:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.embedding_flow:\n",
    "            embeddings = layer(embeddings)\n",
    "        x = x + embeddings[:, None, None]\n",
    "\n",
    "        for layer in self.exit_flow:\n",
    "            x = layer(x)\n",
    "        return x + self.residual_proj(x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Upsample\n",
    "class Upsample(layers.Layer):\n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.upsample = layers.UpSampling2D(2)\n",
    "        self.conv = PaddedConv2D(channels, 3, padding=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.upsample(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CLIPEmbedding\n",
    "class CLIPEmbedding(layers.Layer):\n",
    "    def __init__(self, input_dim=49408, output_dim=768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embedding = layers.Embedding(input_dim, output_dim)\n",
    "        self.position_embedding = layers.Embedding(MAX_PROMPT_LENGTH, output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tokens, positions = inputs\n",
    "        tokens = self.token_embedding(tokens)\n",
    "        positions = self.position_embedding(positions)\n",
    "        return tokens + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CLIPAttention\n",
    "class CLIPAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, causal=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.causal = causal\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.q_proj = layers.Dense(self.embed_dim)\n",
    "        self.k_proj = layers.Dense(self.embed_dim)\n",
    "        self.v_proj = layers.Dense(self.embed_dim)\n",
    "        self.output_proj = layers.Dense(self.embed_dim)\n",
    "\n",
    "    def reshape_states(self, x, sequence_length, batch_size):\n",
    "        x = ops.reshape(x, (batch_size, sequence_length, self.num_heads, self.head_dim))\n",
    "        return ops.transpose(x, (0, 2, 1, 3))  # batch_size, heads, sequence_length, head_dim\n",
    "\n",
    "    def call(self, inputs, attention_mask=None):\n",
    "        if attention_mask is None and self.causal:\n",
    "            length = ops.shape(inputs)[1]\n",
    "            attention_mask = ops.triu(\n",
    "                ops.ones((1, 1, length, length), dtype=self.compute_dtype) * -float(\"inf\"),\n",
    "                k=1,\n",
    "            )\n",
    "\n",
    "        _, tgt_len, embed_dim = inputs.shape\n",
    "        q_states = self.q_proj(inputs) * self.scale\n",
    "        k_states = self.reshape_states(self.k_proj(inputs), tgt_len, -1)\n",
    "        v_states = self.reshape_states(self.v_proj(inputs), tgt_len, -1)\n",
    "\n",
    "        proj_shape = (-1, tgt_len, self.head_dim)\n",
    "        q_states = self.reshape_states(q_states, tgt_len, -1)\n",
    "        q_states = ops.reshape(q_states, proj_shape)\n",
    "        k_states = ops.reshape(k_states, proj_shape)\n",
    "        v_states = ops.reshape(v_states, proj_shape)\n",
    "\n",
    "        src_len = tgt_len\n",
    "        attn_weights = q_states @ ops.transpose(k_states, (0, 2, 1))\n",
    "        attn_weights = ops.reshape(attn_weights, (-1, self.num_heads, tgt_len, src_len))\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "        attn_weights = ops.reshape(attn_weights, (-1, tgt_len, src_len))\n",
    "        attn_weights = ops.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        attn_output = attn_weights @ v_states\n",
    "        attn_output = ops.reshape(attn_output, (-1, self.num_heads, tgt_len, self.head_dim))\n",
    "        attn_output = ops.transpose(attn_output, (0, 2, 1, 3))\n",
    "        attn_output = ops.reshape(attn_output, (-1, tgt_len, embed_dim))\n",
    "        return self.output_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CLIPEncoderLayer\n",
    "class CLIPEncoderLayer(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=EPSILON)\n",
    "        self.clip_attn = CLIPAttention(embed_dim, num_heads, causal=True)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=EPSILON)\n",
    "        self.fc1 = layers.Dense(embed_dim * 4)\n",
    "        self.fc2 = layers.Dense(embed_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x = self.clip_attn(x)\n",
    "        x = residual + x\n",
    "        residual = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title TextEncoder\n",
    "class TextEncoder(Model):\n",
    "    def __init__(self, vocab_size=49408):\n",
    "        tokens = layers.Input(shape=(MAX_PROMPT_LENGTH,), dtype=\"int32\", name=\"tokens\")\n",
    "        positions = layers.Input(shape=(MAX_PROMPT_LENGTH,), dtype=\"int32\", name=\"positions\")\n",
    "\n",
    "        x = CLIPEmbedding(vocab_size, 768)([tokens, positions])\n",
    "        for _ in range(12):\n",
    "            x = CLIPEncoderLayer(768, 12, activation=quick_gelu)(x)\n",
    "\n",
    "        embedded = layers.LayerNormalization(epsilon=EPSILON)(x)\n",
    "        super().__init__([tokens, positions], embedded, name=None)\n",
    "\n",
    "        weights_path = utils.get_file(\n",
    "            cache_dir=CACHE_DIR,\n",
    "            cache_subdir=CACHE_SUBDIR,\n",
    "            # file_hash=\"4789e63e07c0e54d6a34a29b45ce81ece27060c499a709d556c7755b42bb0dc4\",\n",
    "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\",\n",
    "        )\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ImageEncoder\n",
    "class ImageEncoder(Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            [\n",
    "                layers.Input((None, None, 3)),\n",
    "                PaddedConv2D(128, 3, padding=1),\n",
    "                ResnetBlock(128),\n",
    "                ResnetBlock(128),\n",
    "                PaddedConv2D(128, 3, padding=((0, 1), (0, 1)), strides=2),\n",
    "                ResnetBlock(256),\n",
    "                ResnetBlock(256),\n",
    "                PaddedConv2D(256, 3, padding=((0, 1), (0, 1)), strides=2),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                PaddedConv2D(512, 3, padding=((0, 1), (0, 1)), strides=2),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                AttentionBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                layers.GroupNormalization(epsilon=EPSILON),\n",
    "                layers.Activation(\"swish\"),\n",
    "                PaddedConv2D(8, 3, padding=1),\n",
    "                PaddedConv2D(8, 1),\n",
    "                layers.Lambda(lambda x: x[..., :4] * 0.18215),\n",
    "            ]\n",
    "        )\n",
    "        weights_path = utils.get_file(\n",
    "            cache_dir=CACHE_DIR,\n",
    "            cache_subdir=CACHE_SUBDIR,\n",
    "            # file_hash=\"c60fb220a40d090e0f86a6ab4c312d113e115c87c40ff75d11ffcf380aab7ebb\",\n",
    "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5\",\n",
    "        )\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Decoder\n",
    "class Decoder(Sequential):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__(\n",
    "            [\n",
    "                layers.Input((img_height // 8, img_width // 8, 4)),\n",
    "                layers.Rescaling(1.0 / 0.18215),\n",
    "                PaddedConv2D(4, 1),\n",
    "                PaddedConv2D(512, 3, padding=1),\n",
    "                ResnetBlock(512),\n",
    "                AttentionBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                layers.UpSampling2D(2),\n",
    "                PaddedConv2D(512, 3, padding=1),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                ResnetBlock(512),\n",
    "                layers.UpSampling2D(2),\n",
    "                PaddedConv2D(512, 3, padding=1),\n",
    "                ResnetBlock(256),\n",
    "                ResnetBlock(256),\n",
    "                ResnetBlock(256),\n",
    "                layers.UpSampling2D(2),\n",
    "                PaddedConv2D(256, 3, padding=1),\n",
    "                ResnetBlock(128),\n",
    "                ResnetBlock(128),\n",
    "                ResnetBlock(128),\n",
    "                layers.GroupNormalization(epsilon=1e-5),\n",
    "                layers.Activation(\"swish\"),\n",
    "                PaddedConv2D(3, 3, padding=1),\n",
    "            ],\n",
    "        )\n",
    "        weights_path = utils.get_file(\n",
    "            cache_dir=CACHE_DIR,\n",
    "            cache_subdir=CACHE_SUBDIR,\n",
    "            # file_hash=\"ad350a65cc8bc4a80c8103367e039a3329b4231c2469a1093869a345f55b1962\",\n",
    "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\",\n",
    "        )\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DiffusionModel\n",
    "class DiffusionModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_height,\n",
    "        img_width,\n",
    "    ):\n",
    "        context = layers.Input((MAX_PROMPT_LENGTH, 768), name=\"context\")\n",
    "        t_embed_input = layers.Input((320,), name=\"timestep_embedding\")\n",
    "        latent = layers.Input((img_height // 8, img_width // 8, 4), name=\"latent\")\n",
    "        t_emb = layers.Dense(1280)(t_embed_input)\n",
    "        t_emb = layers.Activation(\"swish\")(t_emb)\n",
    "        t_emb = layers.Dense(1280)(t_emb)\n",
    "\n",
    "        # Downsampling flow\n",
    "        outputs = []\n",
    "        x = PaddedConv2D(320, kernel_size=3, padding=1)(latent)\n",
    "        outputs.append(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            x = ResBlock(320)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 40, fully_connected=False)([x, context])\n",
    "            outputs.append(x)\n",
    "        x = PaddedConv2D(320, 3, strides=2, padding=1)(x)  # Downsample 2x\n",
    "        outputs.append(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            x = ResBlock(640)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 80, fully_connected=False)([x, context])\n",
    "            outputs.append(x)\n",
    "        x = PaddedConv2D(640, 3, strides=2, padding=1)(x)  # Downsample 2x\n",
    "        outputs.append(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            x = ResBlock(1280)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 160, fully_connected=False)([x, context])\n",
    "            outputs.append(x)\n",
    "        x = PaddedConv2D(1280, 3, strides=2, padding=1)(x)  # Downsample 2x\n",
    "        outputs.append(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            x = ResBlock(1280)([x, t_emb])\n",
    "            outputs.append(x)\n",
    "\n",
    "        # Middle flow\n",
    "        x = ResBlock(1280)([x, t_emb])\n",
    "        x = SpatialTransformer(8, 160, fully_connected=False)([x, context])\n",
    "        x = ResBlock(1280)([x, t_emb])\n",
    "\n",
    "        # Upsampling flow\n",
    "        for _ in range(3):\n",
    "            x = layers.Concatenate()([x, outputs.pop()])\n",
    "            x = ResBlock(1280)([x, t_emb])\n",
    "        x = Upsample(1280)(x)\n",
    "\n",
    "        for _ in range(3):\n",
    "            x = layers.Concatenate()([x, outputs.pop()])\n",
    "            x = ResBlock(1280)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 160, fully_connected=False)([x, context])\n",
    "        x = Upsample(1280)(x)\n",
    "\n",
    "        for _ in range(3):\n",
    "            x = layers.Concatenate()([x, outputs.pop()])\n",
    "            x = ResBlock(640)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 80, fully_connected=False)([x, context])\n",
    "        x = Upsample(640)(x)\n",
    "\n",
    "        for _ in range(3):\n",
    "            x = layers.Concatenate()([x, outputs.pop()])\n",
    "            x = ResBlock(320)([x, t_emb])\n",
    "            x = SpatialTransformer(8, 40, fully_connected=False)([x, context])\n",
    "\n",
    "        # Exit flow\n",
    "        x = layers.GroupNormalization(epsilon=EPSILON)(x)\n",
    "        x = layers.Activation(\"swish\")(x)\n",
    "        output = PaddedConv2D(4, kernel_size=3, padding=1)(x)\n",
    "\n",
    "        super().__init__([latent, t_embed_input, context], output, name=None)\n",
    "\n",
    "        weights_path = utils.get_file(\n",
    "            cache_dir=CACHE_DIR,\n",
    "            cache_subdir=CACHE_SUBDIR,\n",
    "            # file_hash=\"8799ff9763de13d7f30a683d653018e114ed24a6a819667da4f5ee10f9e805fe\",\n",
    "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\",\n",
    "        )\n",
    "        self.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title StableDiffusion\n",
    "class StableDiffusion:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        img_height: int, height of the images to generate, in pixels. Note that only multiples of 128 are supported; the value provided will be rounded to the nearest valid value. Defaults to 512.\n",
    "        img_width: int, width of the images to generate, in pixels. Note that only multiples of 128 are supported; the value provided will be rounded to the nearest valid value. Defaults to 512.\n",
    "        batch_size: int, number of images to generate. Defaults to 1.\n",
    "        jit_compile: bool or \"auto\", whether to compile the underlying models to XLA. This can lead to a significant speedup on some systems. Defaults to \"auto\".\n",
    "    Example:\n",
    "    ```python\n",
    "    from PIL import Image\n",
    "    model = StableDiffusion()\n",
    "    img = model.text_to_image(prompt=\"A beautiful horse running through a field\", seed=42)\n",
    "    Image.fromarray(img[0]).save(\"horse.png\")\n",
    "    print(\"saved at horse.png\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=1, img_height=512, img_width=512, jit_compile=\"auto\"):\n",
    "        # UNet requires multiples of 2^7 (128)\n",
    "        img_height = round(img_height / 128) * 128\n",
    "        img_width = round(img_width / 128) * 128\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.diffusion_model = DiffusionModel(img_height, img_width)\n",
    "        self.decoder = Decoder(img_height, img_width)\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer()\n",
    "        self.scheduler = DDPMScheduler(\n",
    "            beta_end=0.012,\n",
    "            beta_start=0.00085,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "        )  # from tinygrad\n",
    "\n",
    "        self.image_encoder.compile(jit_compile=jit_compile)\n",
    "        self.text_encoder.compile(jit_compile=jit_compile)\n",
    "        self.diffusion_model.compile(jit_compile=jit_compile)\n",
    "        self.decoder.compile(jit_compile=jit_compile)\n",
    "\n",
    "    def text_to_image(\n",
    "        self,\n",
    "        prompt,\n",
    "        negative_prompt=None,\n",
    "        num_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        seed=None,\n",
    "    ):\n",
    "        encoded_text = self.encode_text(prompt)\n",
    "        return self.generate_image(\n",
    "            encoded_text,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_steps=num_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    def encode_text(self, prompt):\n",
    "        \"\"\"\n",
    "        Encodes a prompt into a latent text encoding.\n",
    "        The encoding produced by this method should be used as the `encoded_text` parameter of `StableDiffusion.generate_image`.\n",
    "        Encoding text separately from generating an image can be used to arbitrarily modify the text encoding prior to image generation, e.g. for walking between two prompts.\n",
    "        Args:\n",
    "            prompt: a string to encode, must be 77 tokens or shorter.\n",
    "        Example:\n",
    "        ```python\n",
    "        model = StableDiffusion()\n",
    "        encoded_text = model.encode_text(\"Tacos at dawn\")\n",
    "        img = model.generate_image(encoded_text)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer.encode(prompt)\n",
    "        if len(inputs) > MAX_PROMPT_LENGTH:\n",
    "            raise ValueError(f\"Prompt is too long (should be <= {MAX_PROMPT_LENGTH} tokens)\")\n",
    "\n",
    "        # pad to max tokens\n",
    "        phrase = inputs + [49407] * (MAX_PROMPT_LENGTH - len(inputs))\n",
    "        phrase = ops.convert_to_tensor([phrase], dtype=\"int32\")\n",
    "\n",
    "        # context\n",
    "        return self.text_encoder.predict_on_batch(\n",
    "            {\"tokens\": phrase, \"positions\": self._get_pos_ids()}\n",
    "        )\n",
    "\n",
    "    def generate_image(\n",
    "        self,\n",
    "        encoded_text,\n",
    "        negative_prompt=None,\n",
    "        num_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates an image based on encoded text.\n",
    "        The encoding passed to this method should be derived from `StableDiffusion.encode_text`.\n",
    "        Args:\n",
    "            encoded_text: Tensor of shape (`batch_size`, 77, 768), or a Tensor of shape (77, 768). When the batch axis is omitted, the same encoded text will be used to produce every generated image.\n",
    "            negative_prompt: a string containing information to negatively guide the image generation (e.g. by removing or altering certain aspects of the generated image), defaults to None.\n",
    "            num_steps: int, number of diffusion steps (controls image quality), defaults to 50.\n",
    "            guidance_scale: float, controlling how closely the image should adhere to the prompt. Larger values result in more closely adhering to the prompt, but will make the image noisier. Defaults to 7.5.\n",
    "            seed: integer which is used to seed the random generation of diffusion noise.\n",
    "        Example:\n",
    "        ```python\n",
    "        batch_size = 8\n",
    "        model = StableDiffusion()\n",
    "        e_tacos = model.encode_text(\"Tacos at dawn\")\n",
    "        e_watermelons = model.encode_text(\"Watermelons at dusk\")\n",
    "        e_interpolated = keras.ops.linspace(e_tacos, e_watermelons, batch_size)\n",
    "        images = model.generate_image(e_interpolated)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        context = self._expand_tensor(encoded_text)\n",
    "\n",
    "        if negative_prompt is None:\n",
    "            unconditional_context = ops.repeat(\n",
    "                self._get_unconditional_context(),\n",
    "                self.batch_size,\n",
    "                axis=0,\n",
    "            )\n",
    "        else:\n",
    "            unconditional_context = self.encode_text(negative_prompt)\n",
    "            unconditional_context = self._expand_tensor(unconditional_context)\n",
    "\n",
    "        latent = self._get_initial_diffusion_noise(seed)\n",
    "\n",
    "        # iterative reverse diffusion stage\n",
    "        num_timesteps = 1000\n",
    "        ratio = (num_timesteps - 1) / (num_steps - 1) if num_steps > 1 else num_timesteps\n",
    "        timesteps = (np.arange(0, num_steps) * ratio).round().astype(np.int64)\n",
    "        alphas, alphas_prev = self._get_initial_alphas(timesteps)\n",
    "        progbar = utils.Progbar(len(timesteps))\n",
    "        iteration = 0\n",
    "\n",
    "        for index, timestep in list(enumerate(timesteps))[::-1]:\n",
    "            latent_prev = latent  # set aside the previous latent vector\n",
    "            t_emb = self._get_timestep_embedding(timestep)\n",
    "            unconditional_latent = self.diffusion_model.predict_on_batch(\n",
    "                {\n",
    "                    \"latent\": latent,\n",
    "                    \"timestep_embedding\": t_emb,\n",
    "                    \"context\": unconditional_context,\n",
    "                }\n",
    "            )\n",
    "            latent = self.diffusion_model.predict_on_batch(\n",
    "                {\n",
    "                    \"latent\": latent,\n",
    "                    \"timestep_embedding\": t_emb,\n",
    "                    \"context\": context,\n",
    "                }\n",
    "            )\n",
    "            latent = latent - unconditional_latent\n",
    "            latent = ops.array(unconditional_latent + guidance_scale * latent)\n",
    "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "            target_dtype = latent_prev.dtype  # keras backend array need to cast explicitly\n",
    "            latent = ops.cast(latent, target_dtype)\n",
    "            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(a_t)\n",
    "            latent = ops.array(latent) * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0\n",
    "            iteration += 1\n",
    "            progbar.update(iteration)\n",
    "\n",
    "        # decoding stage\n",
    "        decoded = self.decoder.predict_on_batch(latent)\n",
    "        decoded = ((decoded + 1) / 2) * 255\n",
    "        return np.clip(decoded, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    def _get_unconditional_context(self):\n",
    "        unconditional_tokens = [49406] + [49407] * (MAX_PROMPT_LENGTH - 1)\n",
    "        unconditional_tokens = ops.convert_to_tensor([unconditional_tokens], dtype=\"int32\")\n",
    "        unconditional_context = self.text_encoder.predict_on_batch(\n",
    "            {\n",
    "                \"tokens\": unconditional_tokens,\n",
    "                \"positions\": self._get_pos_ids(),\n",
    "            }\n",
    "        )\n",
    "        return unconditional_context\n",
    "\n",
    "    def _expand_tensor(self, text_embedding):\n",
    "        text_embedding = ops.squeeze(text_embedding)\n",
    "        if len(text_embedding.shape) == 2:\n",
    "            text_embedding = ops.repeat(\n",
    "                ops.expand_dims(text_embedding, axis=0),\n",
    "                self.batch_size,\n",
    "                axis=0,\n",
    "            )\n",
    "        return text_embedding\n",
    "\n",
    "    def _get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        range = ops.cast(ops.arange(0, half), \"float32\")\n",
    "        freqs = ops.exp(-math.log(max_period) * range / half)\n",
    "        args = ops.convert_to_tensor([timestep], dtype=\"float32\") * freqs\n",
    "        embedding = ops.concatenate([ops.cos(args), ops.sin(args)], 0)\n",
    "        embedding = ops.reshape(embedding, [1, -1])\n",
    "        return ops.repeat(embedding, self.batch_size, axis=0)\n",
    "\n",
    "    def _get_initial_alphas(self, timesteps):\n",
    "        cumprod = self.scheduler.alphas_cumprod\n",
    "        alphas = [cumprod[t] for t in timesteps]\n",
    "        alphas_prev = [1.0] + alphas[:-1]\n",
    "        return alphas, alphas_prev\n",
    "\n",
    "    def _get_initial_diffusion_noise(self, seed):\n",
    "        return random.normal(\n",
    "            (self.batch_size, self.img_height // 8, self.img_width // 8, 4),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pos_ids():\n",
    "        return ops.expand_dims(ops.arange(MAX_PROMPT_LENGTH, dtype=\"int32\"), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_dtype_policy(GLOBAL_DTYPE)\n",
    "model = StableDiffusion(batch_size=BATCH_SIZE, img_height=IMG_HEIGHT, img_width=IMG_WIDTH)  # ~2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.text_to_image(\n",
    "    \"cute corgi at the beach, light sand, blue waves\",\n",
    "    negative_prompt=\"deformed, clutter\",\n",
    ")\n",
    "display_images(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
