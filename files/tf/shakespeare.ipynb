{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Character Prediction (TensorFlow)\n",
    "\n",
    "[![Open in Colab](https://lab.aef.me/files/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/tf/shakespeare.ipynb)\n",
    "[![Open in Kaggle](https://lab.aef.me/files/assets/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/adamelliotfields/lab/blob/main/files/tf/shakespeare.ipynb)\n",
    "[![Render nbviewer](https://lab.aef.me/files/assets/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/tf/shakespeare.ipynb)\n",
    "[![W&B](https://img.shields.io/badge/Weights_&_Biases-FFCC33?logo=WeightsAndBiases&logoColor=black)](https://wandb.ai/adamelliotfields/shakespeare)\n",
    "[![Model on HF](https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md-dark.svg)](https://huggingface.co/adamelliotfields/shakespeare)\n",
    "\n",
    "Tiny Shakespeare is a dataset created by Andrej Karpathy in his blog post, [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "This notebook includes training a LSTM model on the dataset, logging to W&B, and pushing the model to Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "\n",
    "if not find_spec(\"wandb\"):\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"wandb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/\"\n",
    "    os.environ[\"WANDB_DISABLE_GIT\"] = \"true\"\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "    os.environ[\"TFDS_DATA_DIR\"] = f\"{SAVE_DIR}tensorflow_datasets\"\n",
    "    os.environ[\"HF_HUB_CACHE\"] = \"/content/drive/MyDrive/huggingface/hub\"\n",
    "except ImportError:\n",
    "    SAVE_DIR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import wandb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from huggingface_hub import from_pretrained_keras, push_to_hub_keras\n",
    "from wandb.integration.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu,utilization.memory --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "VERBOSE = 1\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 1024\n",
    "SEQUENCE_LENGTH = 100\n",
    "WEIGHT_DECAY = 0.001\n",
    "LEARNING_RATE = 0.00025\n",
    "\n",
    "# NOTE: same username and project on wandb and huggingface\n",
    "WANDB_PROJECT = \"shakespeare\"\n",
    "WANDB_ENTITY = \"adamelliotfields\"\n",
    "\n",
    "# TODO: loading keras models is very slow; serialize however huggingface does\n",
    "MODEL_FILENAME = \"lstm-shakespeare.model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(dataset):\n",
    "    text = dataset.map(lambda x: x[\"text\"]).take(1)\n",
    "    return next(iter(text)).numpy().decode(\"utf8\")\n",
    "\n",
    "\n",
    "(shakespeare_train, shakespeare_test, shakespeare_validation), shakespeare_info = tfds.load(\n",
    "    \"tiny_shakespeare\",\n",
    "    with_info=True,\n",
    "    as_supervised=False,\n",
    "    split=[\"train\", \"test\", \"validation\"],\n",
    ")\n",
    "\n",
    "shakespeare_text = (\n",
    "    extract_text(shakespeare_train)\n",
    "    + extract_text(shakespeare_validation)\n",
    "    + extract_text(shakespeare_test)\n",
    ")\n",
    "\n",
    "# head 2\n",
    "print(\"\\n\".join(shakespeare_text.split(\"\\n\")[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_speeches = shakespeare_text.split(\"\\n\\n\")\n",
    "shakespeare_speeches = [speech.replace(\"\\n\", \" \") for speech in shakespeare_speeches]\n",
    "speech_lengths = [len(speech) for speech in shakespeare_speeches]\n",
    "\n",
    "print(f\"Min: {np.min(speech_lengths)}\")  # 4\n",
    "print(f\"Max: {np.max(speech_lengths)}\")  # 3080\n",
    "print(f\"Mean: {np.mean(speech_lengths):.2f}\")  # 152.44\n",
    "print(f\"Median: {np.median(speech_lengths)}\")  # 83\n",
    "print(f\"Total: {len(speech_lengths)}\")  # 7222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(text):\n",
    "    sequences = []\n",
    "    characters = []\n",
    "    for i in range(0, len(text) - SEQUENCE_LENGTH):\n",
    "        sequences.append(text[i : i + SEQUENCE_LENGTH])\n",
    "        characters.append(text[i + SEQUENCE_LENGTH])\n",
    "    return sequences, characters\n",
    "\n",
    "\n",
    "sequences, characters = create_sequences(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.layers.TextVectorization(standardize=None, split=\"character\", name=\"encoder\")\n",
    "encoder.adapt(tf.constant([shakespeare_text]))\n",
    "\n",
    "# TODO: ideally the encoder would be a layer in the model\n",
    "vocab = encoder.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30-40s\n",
    "X = encoder(sequences)  # 1115294\n",
    "y = encoder(characters)  # 1115294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "ds = ds.shuffle(100000)\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "if os.path.exists(f\"{SAVE_DIR}{MODEL_FILENAME}\"):\n",
    "    model = keras.models.load_model(f\"{SAVE_DIR}{MODEL_FILENAME}\")\n",
    "    # model = from_pretrained_keras(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "else:\n",
    "    x_inputs = keras.Input(shape=(None,), dtype=tf.int64, name=\"input\")\n",
    "    x = keras.layers.Embedding(len(vocab), 512, mask_zero=True, name=\"embedding\")(x_inputs)\n",
    "    x = keras.layers.LSTM(512, return_sequences=True, name=\"lstm1\")(x)\n",
    "    x = keras.layers.Dropout(0.5, name=\"dropout1\")(x)\n",
    "    x = keras.layers.LSTM(512, return_sequences=True, name=\"lstm2\")(x)\n",
    "    x = keras.layers.Dropout(0.5, name=\"dropout2\")(x)\n",
    "    x = keras.layers.LSTM(512, name=\"lstm3\")(x)\n",
    "    x = keras.layers.Dropout(0.5, name=\"dropout3\")(x)\n",
    "    x = keras.layers.Dense(len(vocab), name=\"output\")(x)\n",
    "\n",
    "    model = keras.Model(x_inputs, outputs=x, name=\"LSTM-Shakespeare\")\n",
    "    model.compile(\n",
    "        metrics=[\"accuracy\"],\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train\n",
    "with wandb.init(\n",
    "    tags=[\"L4\"],\n",
    "    group=\"lstm\",\n",
    "    job_type=\"train\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    config={\"dropout\": DROPOUT, \"learning_rate\": LEARNING_RATE, \"weight_decay\": WEIGHT_DECAY},\n",
    ") as run:\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"model\": \"LSTM-Shakespeare\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 5m on L4, 20m on T4\n",
    "    model.fit(\n",
    "        ds,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=VERBOSE,\n",
    "        callbacks=[WandbMetricsLogger(log_freq=\"epoch\")],\n",
    "    )\n",
    "\n",
    "    model.save(f\"{SAVE_DIR}{MODEL_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Huggingface\n",
    "COMMIT_MESSAGE = \"Initial commit\"\n",
    "\n",
    "push_to_hub_keras(\n",
    "    model,\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n",
    "    include_optimizer=True,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    commit_message=COMMIT_MESSAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Predict\n",
    "def generate_text(m, enc, txt, n):\n",
    "    vocab = enc.get_vocabulary()\n",
    "    generated_txt = txt\n",
    "    for _ in range(n):\n",
    "        encoded = enc([generated_txt])\n",
    "        # not stateful so don't need to reset_states first\n",
    "        pred = m.predict(encoded, verbose=0)\n",
    "        pred = tf.squeeze(tf.argmax(pred, axis=-1)).numpy()\n",
    "        generated_txt += vocab[pred]\n",
    "    return generated_txt\n",
    "\n",
    "\n",
    "sample = \"M\"\n",
    "print(generate_text(model, encoder, sample, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
