{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (TensorFlow)\n",
    "\n",
    "[![Open in Colab](https://lab.aef.me/files/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/tf/rnn.ipynb)\n",
    "[![Open in Kaggle](https://lab.aef.me/files/assets/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/adamelliotfields/lab/blob/main/files/tf/rnn.ipynb)\n",
    "[![Render nbviewer](https://lab.aef.me/files/assets/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/tf/rnn.ipynb)\n",
    "\n",
    "Recurrent Neural Networks (RNN) differ from feed-forward networks in that they have loops within their architecture, allowing information from previous steps to influence the current operation.\n",
    "\n",
    "RNNs can \"remember\" the previous words in a sentence to provide context for understanding the current word. This makes RNNs powerful for tasks involving sequential data like language modeling and time series forecasting.\n",
    "\n",
    "Ilya Sutskever's PhD thesis from 2013 was on [_Training Recurrent Neural Networks_](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf).\n",
    "\n",
    "This notebook includes my notes on RNNs as well as examples of variants like LSTMs, GRUs, and BiRNNs using the [Air Passengers](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/AirPassengers) and [IMDB Reviews](https://ai.stanford.edu/~amaas/data/sentiment/) datasets.\n",
    "\n",
    "> If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs. &mdash; [Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "## Backpropagation Through Time\n",
    "\n",
    "A regular deep neural network has many layers with different weights. When we backpropagate through the network, we calculate the gradients of the loss function for each weight in each layer.\n",
    "\n",
    "The number of \"loops\" in an RNN is equal to the number of time steps in the sequence. If the input is 100 characters, then there are 100 steps. Each step essentially forms a new layer that is connected to the previous step. To train a RNN, we need to \"unroll\" all these steps, which forms a deep feed-forward network, except the weights are shared across all the steps. This is known as BPTT (backpropagation through time).\n",
    "\n",
    "In Keras, this is all handled transparently.\n",
    "\n",
    "## Statefulness\n",
    "\n",
    "When batching, the state at the start of a batch is essentially reset. If you need to maintain state across batches, you can set `stateful=True`, which initializes the state of the batch to the previous state of the last batch. This is necessary when you cannot fit the necessary context in the sequences in the batch.\n",
    "\n",
    "## Bidirectional\n",
    "\n",
    "Bidirectional RNNs are a variant that introduce two hidden states, one that processes the sequence from left to right and another that processes the sequence from right to left. This allows the network to learn from both \"past\" and \"future\" data. The outputs of each are combined, which allows the model to capture information from both directions.\n",
    "\n",
    "## Long Short-Term Memory\n",
    "\n",
    "Because RNN layers form very deep networks when unrolled, they suffer from the same gradient stability problems as other very deep networks. This makes it difficult for traditional RNNs to learn long-range dependencies in the data.\n",
    "\n",
    "LSTM (long short-term memory) networks (Hochreiter and Schmidhuber, [1997](https://www.bioinf.jku.at/publications/older/2604.pdf)) introduce memory cells and gating mechanisms to selectively remember and forget information. There are 3 gates in an LSTM network:\n",
    "1. Input: decides what new information to store in the cell state.\n",
    "2. Forget: decides what information to throw away from the cell state.\n",
    "3. Output: decides what to output based on input and the memory of the cell.\n",
    "\n",
    "## Gated Recurrent Unit\n",
    "\n",
    "GRU networks (Cho et al., [2014](https://arxiv.org/abs/1406.1078)) are a simplified version of LSTMs that combine the forget and input gates into a single update gate.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* [wikipedia.org/wiki/recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "* [wikipedia.org/wiki/long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "* [wikipedia.org/wiki/gated_recurrent_unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n",
    "* [deeplearningbook.org/contents/rnn](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "* [developer.ibm.com/articles/cc-cognitive-recurrent-neural-networks](https://developer.ibm.com/articles/cc-cognitive-recurrent-neural-networks/)\n",
    "* [neptune.ai/blog/recurrent-neural-network-guide](https://neptune.ai/blog/recurrent-neural-network-guide)\n",
    "* [blog.paperspace.com/bidirectional-rnn-keras](https://blog.paperspace.com/bidirectional-rnn-keras/)\n",
    "* [stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "* [colah.github.io/posts/2015-08-understanding-lstms](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [joshvarty.github.io/visualizingrnns](https://joshvarty.github.io/VisualizingRNNs/)\n",
    "* [machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras](https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/)\n",
    "* [bouvet.no/bouvet-deler/explaining-recurrent-neural-networks](https://www.bouvet.no/bouvet-deler/explaining-recurrent-neural-networks)\n",
    "* [tensorflow.org/guide/keras/working_with_rnns](https://www.tensorflow.org/guide/keras/working_with_rnns)\n",
    "* [pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib.util import find_spec\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "if find_spec(\"google.colab\"):\n",
    "    os.environ[\"TFDS_DATA_DIR\"] = \"/content/drive/MyDrive/tensorflow_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Air Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts fractional years to datetimes\n",
    "def parse_time(d):\n",
    "    year = int(d)\n",
    "    month = d - year\n",
    "    month = round(month * 12) + 1\n",
    "    return datetime(year, month, 1)\n",
    "\n",
    "\n",
    "def get_sequences(data, length=12):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - length):\n",
    "        sequences.append(data[i : i + length])\n",
    "        labels.append(data[i + length])\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.rdocumentation.org/packages/datasets/topics/AirPassengers\n",
    "air_df = sm.datasets.get_rdataset(\"AirPassengers\", cache=True).data\n",
    "air_df.columns = [\"ds\", \"y\"]\n",
    "air_df[\"ds\"] = air_df[\"ds\"].apply(parse_time)\n",
    "air_df.set_index(\"ds\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "air_df_scaled = air_df.copy()\n",
    "air_df_scaled[\"y\"] = scaler.fit_transform(air_df_scaled[[\"y\"]])\n",
    "\n",
    "# generates 132 sequences of 12 (144 - 12 = 132)\n",
    "X, y = get_sequences(air_df_scaled.y.values, length=12)\n",
    "\n",
    "# split into train, test, and validation sets\n",
    "X_train, y_train = X[:-36], y[:-36]\n",
    "X_test, y_test = X[-24:], y[-24:]\n",
    "X_val, y_val = X[-36:-24], y[-36:-24]\n",
    "\n",
    "# reshape for RNN input layer\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear model cache\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# build model\n",
    "# when stacking, return sequences all but the last\n",
    "x_input = keras.Input(shape=(12, 1))\n",
    "x = keras.layers.SimpleRNN(\n",
    "    32,\n",
    "    activation=\"relu\",\n",
    "    recurrent_dropout=0.1,\n",
    "    return_sequences=True,\n",
    ")(x_input)\n",
    "x = keras.layers.SimpleRNN(32, activation=\"relu\", recurrent_dropout=0.1)(x)\n",
    "x = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "rnn_model = keras.Model(x_input, outputs=x, name=\"AirPassengersRNN\")\n",
    "rnn_model.compile(optimizer=\"adam\", loss=\"mse\")  # 3201 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "rnn_model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform the predictions and actual values\n",
    "y_pred = rnn_model.predict(X_test, verbose=0)\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(air_df.index[-24:], y_test_inv, label=\"Actual\")\n",
    "plt.plot(air_df.index[-24:], y_pred_inv, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "x_input = keras.Input(shape=(12, 1))\n",
    "x = keras.layers.LSTM(32, activation=\"relu\", recurrent_dropout=0.1, return_sequences=True)(x_input)\n",
    "x = keras.layers.LSTM(32, activation=\"relu\", recurrent_dropout=0.1)(x)\n",
    "x = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "lstm_model = keras.Model(x_input, outputs=x, name=\"AirPassengersLSTM\")\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"mse\")  # 12705 params\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires more epochs and normalization\n",
    "lstm_model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform the predictions and actual values\n",
    "y_pred = lstm_model.predict(X_test, verbose=0)\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(air_df.index[-24:], y_test_inv, label=\"Actual\")\n",
    "plt.plot(air_df.index[-24:], y_pred_inv, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "x_input = keras.Input(shape=(12, 1))\n",
    "x = keras.layers.GRU(32, activation=\"relu\", recurrent_dropout=0.1, return_sequences=True)(x_input)\n",
    "x = keras.layers.GRU(32, activation=\"relu\", recurrent_dropout=0.1)(x)\n",
    "x = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "gru_model = keras.Model(x_input, outputs=x, name=\"AirPassengersLSTM\")\n",
    "gru_model.compile(optimizer=\"adam\", loss=\"mse\")  # 9729 params\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru requires more epochs to be competitive with LSTM\n",
    "gru_model.fit(X_train, y_train, epochs=400, validation_data=(X_val, y_val), verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform the predictions and actual values\n",
    "y_pred = gru_model.predict(X_test, verbose=0)\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(air_df.index[-24:], y_test_inv, label=\"Actual\")\n",
    "plt.plot(air_df.index[-24:], y_pred_inv, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(imdb_train, imdb_test, imdb_unsupervised), imdb_info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=(\"train\", \"test\", \"unsupervised\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised keys are \"text\" (str) and \"label\" (int)\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(\"text: \", example.numpy().decode(\"utf8\"))\n",
    "    print(\"label: \", label.numpy())\n",
    "\n",
    "for example, label in imdb_test.take(1):\n",
    "    print(\"text: \", example.numpy().decode(\"utf8\"))\n",
    "    print(\"label: \", label.numpy())\n",
    "\n",
    "# unsupervised set is unlabeled (-1)\n",
    "for example, label in imdb_unsupervised.take(1):\n",
    "    print(\"text: \", example.numpy().decode(\"utf8\"))\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and batch\n",
    "imdb_train = imdb_train.shuffle(buffer_size=imdb_train.cardinality())\n",
    "imdb_train = imdb_train.batch(128)\n",
    "imdb_train = imdb_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "imdb_test = imdb_test.batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "\n",
    "encoder = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(imdb_train.map(lambda text, label: text))  # take only text\n",
    "\n",
    "vocab = np.array(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "x_input = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = encoder(x_input)\n",
    "x = keras.layers.Embedding(\n",
    "    input_dim=len(vocab),\n",
    "    output_dim=64,\n",
    "    mask_zero=True,\n",
    ")(x)  # mask to handle variable sequence lengths\n",
    "x = keras.layers.LSTM(128)(x)\n",
    "x = keras.layers.Dense(64, activation=\"gelu\")(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "imdb_lstm = keras.Model(x_input, outputs=x, name=\"IMDBLSTM\")\n",
    "imdb_lstm.compile(\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),  # no activation on output layer\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        weight_decay=4e-4,\n",
    "        learning_rate=1e-4,  # lower learning rate than default\n",
    "    ),\n",
    ")\n",
    "imdb_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~15s/epoch on T4/L4\n",
    "imdb_lstm.fit(imdb_train, epochs=5, validation_data=imdb_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_lstm.save(\"imdb_lstm.keras\")\n",
    "imdb_lstm = keras.saving.load_model(\"imdb_lstm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = imdb_lstm.evaluate(imdb_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "x_input = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = encoder(x_input)\n",
    "x = keras.layers.Embedding(input_dim=len(vocab), output_dim=64, mask_zero=True)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True))(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(64))(x)\n",
    "x = keras.layers.Dense(64, activation=\"gelu\")(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "imdb_bigru = keras.Model(x_input, outputs=x, name=\"IMDBBiGRU\")\n",
    "imdb_bigru.compile(\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=4e-4),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "imdb_bigru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_bigru.save(\"imdb_bigru.keras\")\n",
    "imdb_bigru = keras.saving.load_model(\"imdb_bigru.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = imdb_bigru.evaluate(imdb_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = imdb_bigru.predict(\n",
    "    np.array([\"This movie was great!\", \"This movie was so bad.\"]),\n",
    "    verbose=0,\n",
    ")\n",
    "for pred in y_preds:\n",
    "    print(f\"Sentiment: {'positive' if pred >= 0.0 else 'negative'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
