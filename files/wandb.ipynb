{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases\n",
    "\n",
    "[![Open in Colab](https://lab.aef.me/files/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/wandb.ipynb)\n",
    "[![Open in Kaggle](https://lab.aef.me/files/assets/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/adamelliotfields/lab/blob/main/files/wandb.ipynb)\n",
    "[![Render nbviewer](https://lab.aef.me/files/assets/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/wandb.ipynb)\n",
    "\n",
    "Experiment tracking with [W&B](https://wandb.ai).\n",
    "\n",
    "This notebook starts with a couple Scikit-learn estimators to demonstrate the basics followed by a couple Keras models to demonstrate model checkpointing and hyperparameter sweeps.\n",
    "\n",
    "**Resources**\n",
    "\n",
    "* [Alerts](https://docs.wandb.ai/guides/runs/alert)\n",
    "* [Environment variables](https://docs.wandb.ai/guides/track/environment-variables)\n",
    "* Artifacts:\n",
    "  - [TTL](https://docs.wandb.ai/guides/artifacts/ttl)\n",
    "  - [Webhooks](https://docs.wandb.ai/guides/artifacts/project-scoped-automations)\n",
    "* Integrations:\n",
    "  - [ðŸ¤— Transformers](https://docs.wandb.ai/guides/integrations/huggingface)\n",
    "  - [ðŸ¤— Diffusers](https://docs.wandb.ai/guides/integrations/diffusers)\n",
    "  - [Keras](https://docs.wandb.ai/guides/integrations/keras)\n",
    "  - [TensorBoard](https://docs.wandb.ai/guides/integrations/tensorboard)\n",
    "  - [Lightning](https://docs.wandb.ai/guides/integrations/lightning)\n",
    "  - [LightGBM](https://docs.wandb.ai/guides/integrations/lightgbm)\n",
    "  - [Sklearn](https://docs.wandb.ai/guides/integrations/scikit)\n",
    "  - [OpenAI](https://docs.wandb.ai/guides/integrations/openai-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    # disable saving notebook if scratchpad\n",
    "    # os.environ[\"WANDB_DISABLE_CODE\"] = \"true\"\n",
    "\n",
    "    os.environ[\"WANDB_DISABLE_GIT\"] = \"true\"\n",
    "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "    os.environ[\"TFDS_DATA_DIR\"] = \"/content/drive/MyDrive/tensorflow_datasets\"\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# can also use `wandb.login` for interactive login\n",
    "assert os.environ.get(\"WANDB_API_KEY\"), \"missing WANDB_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import wandb\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from wandb.sklearn import plot_precision_recall, plot_feature_importances\n",
    "from wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "from keras import (\n",
    "    Input,\n",
    "    Model,\n",
    "    Sequential,\n",
    "    initializers,\n",
    "    layers,\n",
    "    losses,\n",
    "    optimizers,\n",
    ")\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_diabetes, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Config\n",
    "WANDB_ENTITY = \"adamelliotfields\"  # @param {type:\"string\"}\n",
    "WANDB_PROJECT = \"test\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "iris_df = pd.DataFrame(data=np.c_[X_iris, y_iris], columns=iris.feature_names + [\"target\"])\n",
    "iris_df.target = pd.Categorical.from_codes(y_iris, iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    iris_df,\n",
    "    color=\"target\",\n",
    "    trendline=\"ols\",\n",
    "    marginal_x=\"box\",\n",
    "    marginal_y=\"violin\",\n",
    "    x=\"sepal width (cm)\",\n",
    "    y=\"sepal length (cm)\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = DecisionTreeClassifier()\n",
    "# classifier = RandomForestClassifier(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=42)\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_iris_train, y_iris_train)\n",
    "y_probas = classifier.predict_proba(X_iris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellowbrick confusion matrix\n",
    "cm = ConfusionMatrix(classifier, classes=iris.target_names, cmap=\"Blues\", is_fitted=True)\n",
    "cm.fit(X_iris_train, y_iris_train)\n",
    "cm.score(X_iris_test, y_iris_test)\n",
    "\n",
    "# save as PIL image\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format=\"png\")\n",
    "plt.show()\n",
    "buf.seek(0)\n",
    "img = PILImage.open(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a run instance, which can also be accessed on `wandb.run`\n",
    "wandb.init(\n",
    "    group=\"iris\",\n",
    "    tags=[\"CPU\"],\n",
    "    job_type=\"train\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    notes=\"KNN classifier\",\n",
    "    config=classifier.get_params(),\n",
    ")\n",
    "\n",
    "# log additional information\n",
    "wandb.config.update(\n",
    "    {\n",
    "        \"test_size\": 0.2,\n",
    "        \"model\": \"KNeighborsClassifier\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# renders an interactive Plotly figure (in the dashboard)\n",
    "# wandb.log is shorthand for wandb.run.log\n",
    "wandb.log({\"Plotly\": wandb.Plotly(fig)})\n",
    "\n",
    "# renders a static image\n",
    "wandb.log({\"Confusion Matrix\": wandb.Image(img)})\n",
    "\n",
    "# create a dataset artifact and additionally attach the raw CSV\n",
    "iris_df.to_csv(\"iris.csv\", index=False)\n",
    "iris_table = wandb.Table(dataframe=iris_df)\n",
    "iris_artifact = wandb.Artifact(\"data\", type=\"dataset\")\n",
    "iris_artifact.add(iris_table, \"table\")\n",
    "iris_artifact.add_file(\"iris.csv\")\n",
    "wandb.log({\"data\": iris_table})\n",
    "wandb.log_artifact(iris_artifact)\n",
    "\n",
    "# built-in wandb plots for scikit-learn\n",
    "plot_class_proportions(y_iris_train, y_iris_test, iris.target_names)\n",
    "plot_learning_curve(classifier, X_iris_train, y_iris_train, random_state=42)\n",
    "plot_roc(y_iris_test, y_probas, iris.target_names)\n",
    "plot_precision_recall(y_iris_test, y_probas, iris.target_names)\n",
    "# plot_feature_importances(classifier, iris.feature_names)  # only for trees\n",
    "\n",
    "# must call finish in a notebook (if not using context)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(\n",
    "    X_diabetes,\n",
    "    y_diabetes,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "diabetes_df = pd.DataFrame(\n",
    "    data=np.c_[X_diabetes, y_diabetes],\n",
    "    columns=diabetes.feature_names + [\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    subsample=0.9,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=10,\n",
    ")\n",
    "\n",
    "regressor.fit(X_diabetes_train, y_diabetes_train)\n",
    "y_pred = regressor.predict(X_diabetes_test)\n",
    "\n",
    "# logging these will automatically plot them\n",
    "r2 = r2_score(y_diabetes_test, y_pred)\n",
    "mse = mean_squared_error(y_diabetes_test, y_pred)\n",
    "mae = mean_absolute_error(y_diabetes_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_diabetes_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a context manager so you don't need to call `finish`\n",
    "with wandb.init(\n",
    "    tags=[\"CPU\"],\n",
    "    job_type=\"train\",\n",
    "    group=\"diabetes\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    config=regressor.get_params(),\n",
    "    notes=\"GradientBoostingRegressor\",\n",
    ") as run:\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"test_size\": 0.2,\n",
    "            \"model\": \"GradientBoostingRegressor\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # use a slash to group\n",
    "    run.log({\"metrics/R2\": r2, \"metrics/MSE\": mse, \"metrics/MAE\": mae, \"metrics/MAPE\": mape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Config\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "VERBOSE = 1\n",
    "DROPOUT = 0.1\n",
    "MAX_FILTERS = 8\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0015\n",
    "ACTIVATION = \"leaky_relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CNN\n",
    "def get_cnn(seed=42, classes=10, dropout=0.1, max_filters=8, activation=\"relu\"):\n",
    "    x_input = Input(shape=(28, 28, 1), name=\"input\")\n",
    "    x = layers.Conv2D(\n",
    "        max_filters // 4,\n",
    "        3,\n",
    "        name=\"conv1\",\n",
    "        padding=\"same\",\n",
    "        activation=activation,\n",
    "    )(x_input)\n",
    "    x = layers.MaxPooling2D(2, name=\"pool1\")(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        max_filters // 2,\n",
    "        3,\n",
    "        name=\"conv2\",\n",
    "        padding=\"same\",\n",
    "        activation=activation,\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D(2, name=\"pool2\")(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        max_filters,\n",
    "        3,\n",
    "        name=\"conv3\",\n",
    "        padding=\"same\",\n",
    "        activation=activation,\n",
    "    )(x)\n",
    "\n",
    "    x = layers.Flatten(name=\"flatten\")(x)\n",
    "    x = layers.Dropout(dropout, name=\"dropout\")(x)\n",
    "    x = layers.Dense(\n",
    "        classes,\n",
    "        name=\"output\",\n",
    "        activation=\"softmax\" if classes > 2 else \"sigmoid\",\n",
    "        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=seed),\n",
    "    )(x)\n",
    "\n",
    "    return Model(x_input, outputs=x, name=\"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train, mnist_test), mnist_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=[\"train\", \"test\"],\n",
    ")\n",
    "\n",
    "# fmt: off\n",
    "X_train = mnist_train.take(55000).shuffle(seed=SEED, buffer_size=mnist_train.cardinality()).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "X_val = mnist_train.skip(55000).take(5000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "X_test = mnist_test.take(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "augment = Sequential([layers.Rescaling(scale=2.0 / 255, offset=-1)], name=\"augment\")\n",
    "# fmt: on\n",
    "\n",
    "cnn = get_cnn(seed=SEED, dropout=DROPOUT, activation=ACTIVATION, max_filters=MAX_FILTERS)\n",
    "\n",
    "x_input = Input(shape=(28, 28, 1), name=\"input\")\n",
    "x = augment(x_input)\n",
    "x = cnn(x)\n",
    "\n",
    "model = Model(inputs=x_input, outputs=x, name=\"CNN-MNIST\")\n",
    "model.compile(\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(\n",
    "    tags=[\"T4\"],\n",
    "    group=\"mnist\",\n",
    "    job_type=\"train\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    notes=\"Run without hyperparameter sweep\",\n",
    "    config={\"activation\": ACTIVATION, \"dropout\": DROPOUT, \"learning_rate\": LEARNING_RATE},\n",
    ") as run:\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"model\": \"CNNClassifier\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=VERBOSE,\n",
    "        validation_data=X_val,\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger(log_freq=\"epoch\"),\n",
    "            # creates {username}/{project}/run_{id}_model:v{epoch}\n",
    "            WandbModelCheckpoint(\n",
    "                \"cnn-mnist-v{epoch}.model.keras\",\n",
    "                verbose=VERBOSE,\n",
    "                monitor=\"accuracy\",\n",
    "                save_best_only=True,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # also upload artifact manually\n",
    "    # creates {username}/{project}/cnn-mnist:v0\n",
    "    model.save(\"cnn-mnist.model.keras\")\n",
    "    artifact = wandb.Artifact(\"cnn-mnist\", type=\"model\")\n",
    "    artifact.add_file(\"cnn-mnist.model.keras\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep\n",
    "\n",
    "The sweep controller runs on W&B's cloud; the agent runs on your machine and gets parameters from the controller. Each sweep has an ID, and you can provide that ID to agents on multiple machines to parallelize the sweep.\n",
    "\n",
    "You can also create a sweep config from an [existing project](https://docs.wandb.ai/guides/sweeps/existing-project) using hyperparameters you've already logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Config\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "VERBOSE = 1\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_LAYERS = 3\n",
    "HIDDEN_UNITS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title MLP\n",
    "def get_mlp(seed=42, classes=10, dropout=0.1, activation=\"relu\", hidden_layers=2, hidden_units=32):\n",
    "    # take 2D as input\n",
    "    x_input = Input(shape=(28, 28, 1), name=\"input\")\n",
    "    x = layers.Flatten(name=\"flatten\")(x_input)\n",
    "\n",
    "    for i in range(hidden_layers):\n",
    "        x = layers.Dense(\n",
    "            hidden_units,\n",
    "            name=f\"dense_{i}\",\n",
    "            activation=activation,\n",
    "        )(x)\n",
    "        x = layers.Dropout(dropout, name=f\"dropout_{i}\")(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        classes,\n",
    "        name=\"output\",\n",
    "        activation=\"softmax\" if classes > 2 else \"sigmoid\",\n",
    "        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=seed),\n",
    "    )(x)\n",
    "\n",
    "    return Model(x_input, outputs=x, name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Trainer\n",
    "def train(config=None):\n",
    "    # project and entity passed to `wandb.sweep` further down\n",
    "    with wandb.init(\n",
    "        tags=[\"CPU\"],\n",
    "        config=config,\n",
    "        group=\"mnist\",\n",
    "        job_type=\"sweep\",\n",
    "        notes=\"Hyperparameter sweep\",\n",
    "    ):\n",
    "        wandb.config.update(\n",
    "            {\n",
    "                \"optimizer\": \"Adam\",\n",
    "                \"model\": \"MLPClassifier\",\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"hidden_units\": HIDDEN_UNITS,\n",
    "                \"hidden_layers\": HIDDEN_LAYERS,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        augment = Sequential([layers.Rescaling(scale=2.0 / 255, offset=-1)], name=\"augment\")\n",
    "        mlp = get_mlp(\n",
    "            seed=SEED,\n",
    "            hidden_units=HIDDEN_UNITS,\n",
    "            hidden_layers=HIDDEN_LAYERS,\n",
    "            dropout=wandb.config.dropout,\n",
    "            activation=wandb.config.activation,\n",
    "        )\n",
    "\n",
    "        x_input = Input(shape=(28, 28, 1), name=\"input\")\n",
    "        x = augment(x_input)\n",
    "        x = mlp(x)\n",
    "\n",
    "        model = Model(inputs=x_input, outputs=x, name=\"MLP-MNIST\")\n",
    "        model.compile(\n",
    "            metrics=[\"accuracy\"],\n",
    "            loss=losses.SparseCategoricalCrossentropy(),\n",
    "            optimizer=optimizers.Adam(learning_rate=wandb.config.learning_rate),\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            verbose=VERBOSE,\n",
    "            validation_data=X_val,\n",
    "            epochs=wandb.config.epochs,\n",
    "            callbacks=[WandbMetricsLogger(log_freq=\"epoch\")],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the sweep controller on W&B and returns the ID\n",
    "sweep_id = wandb.sweep(\n",
    "    {\n",
    "        # \"method\": \"random\",\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\n",
    "            \"name\": \"epoch/val_accuracy\",  # the metric as it appears in the dashboard, not what you pass to `model.compile`\n",
    "            \"goal\": \"maximize\",\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"activation\": {\"values\": [\"relu\", \"leaky_relu\", \"swish\", \"gelu\"]},\n",
    "            \"epochs\": {\"value\": EPOCHS},\n",
    "            \"dropout\": {\"values\": [0.0, 0.2]},\n",
    "            \"learning_rate\": {\"values\": [0.001, 0.002]},\n",
    "            #   \"learning_rate\": {\n",
    "            #       \"distribution\": \"uniform\",\n",
    "            #       \"min\": 0.001,\n",
    "            #       \"max\": 0.1\n",
    "            #   }\n",
    "        },\n",
    "    },\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    ")\n",
    "\n",
    "# connect to controller to get parameters to pass to train function\n",
    "wandb.agent(sweep_id, function=train, count=16)\n",
    "\n",
    "# use CLI to stop sweep\n",
    "# can also pause, resume, and cancel\n",
    "subprocess.run([\"wandb\", \"sweep\", \"--stop\", f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
