{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/torch.ipynb)\n",
    "[![Render nbviewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/torch.ipynb)\n",
    "\n",
    "This is a simple feedforward neural net in PyTorch with Lightning. Lightning allows you to organize your code in a very readable way, handles the training loop, and logs metrics to TensorBoard automatically.\n",
    "\n",
    "Try changing the hyperparameters and architecture to see how it affects the model's performance.\n",
    "\n",
    "> NB: PyTorch is not compatible with JupyterLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [`LightningDataModule`](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) encapsulates all the steps needed to prepare your data for training and wraps the splits in a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisData(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        iris = load_iris()\n",
    "        self.X = iris.data\n",
    "        self.y = iris.target\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.X,\n",
    "            self.y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "        self.train_ds = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.long),\n",
    "        )\n",
    "        self.val_ds = TensorDataset(\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_val, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `save_hyperparameters` to save the hyperparameters on the `hparams` attribute and log to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNet(pl.LightningModule):\n",
    "    def __init__(self, dropout, lr, momentum_decay, weight_decay, gamma):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # normalization isn't necessary because the units are all on the same scale\n",
    "            # dropout layer goes after the activation function\n",
    "            nn.Linear(4, 8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum_decay = momentum_decay\n",
    "        self.gamma = gamma\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        # calculate loss\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        # calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.mean((preds == y).float())\n",
    "        # log both\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    # optimization function and optional learning rate scheduler\n",
    "    def configure_optimizers(self):\n",
    "        # NAdam and AdamW are both competitive here\n",
    "        optimizer = torch.optim.NAdam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum_decay=self.momentum_decay,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer,\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is simply instantiating the `Trainer` and passing the model and data to `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "data = IrisData(\n",
    "    batch_size=8,\n",
    ")\n",
    "model = IrisNet(\n",
    "    dropout=0,\n",
    "    lr=1e-2,\n",
    "    momentum_decay=1e-4,\n",
    "    weight_decay=1e-8,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "devices = 1 if torch.cuda.is_available() else 0\n",
    "accelerator = \"gpu\" if devices else None\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    verbose=False,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
