{"metadata":{"kernelspec":{"display_name":"Python (Pyodide)","language":"python","name":"python"},"language_info":{"codemirror_mode":{"name":"python","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Torch\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/lab/blob/main/files/torch.ipynb)\n[![Render nbviewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/lab/blob/main/files/torch.ipynb)\n\nThis is a simple feedforward neural net in PyTorch with Lightning. Lightning allows you to organize your code in a very readable way, handles the training loop, and logs metrics to TensorBoard automatically.\n\nTry changing the hyperparameters and architecture to see how it affects the model's performance.\n\n> NB: PyTorch is not compatible with JupyterLite.","metadata":{}},{"cell_type":"code","source":"import torch\nimport warnings\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport pytorch_lightning as pl\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A [`LightningDataModule`](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) encapsulates all the steps needed to prepare your data for training and wraps the splits in a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).","metadata":{}},{"cell_type":"code","source":"class IrisData(pl.LightningDataModule):\n    def __init__(self, batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n\n    def prepare_data(self):\n        iris = load_iris()\n        self.X = iris.data\n        self.y = iris.target\n\n    def setup(self, stage=None):\n        X_train, X_val, y_train, y_val = train_test_split(\n            self.X,\n            self.y,\n            test_size=0.2,\n            random_state=42,\n        )\n        self.train_ds = TensorDataset(\n            torch.tensor(X_train, dtype=torch.float32),\n            torch.tensor(y_train, dtype=torch.long),\n        )\n        self.val_ds = TensorDataset(\n            torch.tensor(X_val, dtype=torch.float32),\n            torch.tensor(y_val, dtype=torch.long),\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.train_ds, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_ds, batch_size=self.batch_size)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use `save_hyperparameters` to save the hyperparameters on the `hparams` attribute and log to TensorBoard.","metadata":{}},{"cell_type":"code","source":"class IrisNet(pl.LightningModule):\n    def __init__(self, dropout, lr, momentum_decay, weight_decay, gamma):\n        super().__init__()\n        self.model = nn.Sequential(\n            # normalization isn't necessary because the units are all on the same scale\n            # dropout layer goes after the activation function\n            nn.Linear(4, 8),\n            nn.GELU(),\n            nn.Linear(8, 16),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(16, 3),\n        )\n        self.loss = nn.CrossEntropyLoss()\n        self.dropout = dropout\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.momentum_decay = momentum_decay\n        self.gamma = gamma\n        self.save_hyperparameters()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, _):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        return loss\n\n    def validation_step(self, batch, _):\n        # calculate loss\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        # calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = torch.mean((preds == y).float())\n        # log both\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    # optimization function and optional learning rate scheduler\n    def configure_optimizers(self):\n        # NAdam and AdamW are both competitive here\n        optimizer = torch.optim.NAdam(\n            self.parameters(),\n            lr=self.lr,\n            momentum_decay=self.momentum_decay,\n            weight_decay=self.weight_decay,\n        )\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n            optimizer,\n            gamma=self.gamma,\n        )\n        return [optimizer], [scheduler]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training is simply instantiating the `Trainer` and passing the model and data to `fit`.","metadata":{}},{"cell_type":"code","source":"# train\ndata = IrisData(\n    batch_size=8,\n)\nmodel = IrisNet(\n    dropout=0,\n    lr=1e-2,\n    momentum_decay=1e-4,\n    weight_decay=1e-8,\n    gamma=0.99,\n)\n\ndevices = 1 if torch.cuda.is_available() else 0\naccelerator = \"gpu\" if devices else None\n\nearly_stopping = pl.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    mode=\"min\",\n    verbose=False,\n)\ntrainer = pl.Trainer(\n    max_epochs=50,\n    accelerator=accelerator,\n    devices=devices,\n    callbacks=[early_stopping],\n)\ntrainer.fit(model, data)","metadata":{},"outputs":[],"execution_count":null}]}